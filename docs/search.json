[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Streams of Consciousness",
    "section": "",
    "text": "Research Workflows\n\n\n\n\n\n\npersonal\n\n\nproductivity\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nSaeejith Nair\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Saeejith Nair",
    "section": "",
    "text": "I’m currently an MASc candidate at UWaterloo’s Vision and Image Processing Lab, advised by Prof. Alexander Wong and Prof. Javad Shafiee. My research has focused on projects at the intersection of efficient ML (specifically applying neural architecture search to NeRFs and vision transformers), embedded systems (latency prediction for on-device inference), and robotics (building simulation pipelines for synthetic data generation and training embodied morphologies).\nThough my projects span diverse areas, I view them as interconnected elements, each contributing uniquely to the overarching goal of creating robots endowed with the capability for open-ended learning. However, while today’s simulators are valuable for training tomorrow’s robots, I believe that future agents will require fundamentally different neural architectures to achieve the level of learning and power efficiency seen in biological systems. I’m interested in using physics-based simulations as a testbed to develop agents that can reason in an open-ended manner with the aim of applying those insights to build systems capable of open-ended neural architecture generation.\nI previously graduated from the University of Waterloo with a BASc in Mechatronics Engineering with an Option in Artificial Intelligence. As an undergraduate, I’ve interned as a software engineer across the full systems stack from sensor processing and communications infrastructure on self-driving cars, simulators for robot grasping, real-time video analytics applications for embedded microcontrollers, safety-critical bootloader firmware for aerospace products, silicon validation for AI accelerators, and web development for e-commerce.\nI take pride in being a strong generalist and enjoy working across roles and disciplines. I’m looking for new opportunities to build the next generation (and beyond) of robotics. If you’re working on something interesting, or want to learn more, I’d love to chat!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Saeejith Nair",
    "section": "",
    "text": "I’m currently an MASc candidate at UWaterloo’s Vision and Image Processing Lab, advised by Prof. Alexander Wong and Prof. Javad Shafiee. My research has focused on projects at the intersection of efficient ML (specifically applying neural architecture search to NeRFs and vision transformers), embedded systems (latency prediction for on-device inference), and robotics (building simulation pipelines for synthetic data generation and training embodied morphologies).\nThough my projects span diverse areas, I view them as interconnected elements, each contributing uniquely to the overarching goal of creating robots endowed with the capability for open-ended learning. However, while today’s simulators are valuable for training tomorrow’s robots, I believe that future agents will require fundamentally different neural architectures to achieve the level of learning and power efficiency seen in biological systems. I’m interested in using physics-based simulations as a testbed to develop agents that can reason in an open-ended manner with the aim of applying those insights to build systems capable of open-ended neural architecture generation.\nI previously graduated from the University of Waterloo with a BASc in Mechatronics Engineering with an Option in Artificial Intelligence. As an undergraduate, I’ve interned as a software engineer across the full systems stack from sensor processing and communications infrastructure on self-driving cars, simulators for robot grasping, real-time video analytics applications for embedded microcontrollers, safety-critical bootloader firmware for aerospace products, silicon validation for AI accelerators, and web development for e-commerce.\nI take pride in being a strong generalist and enjoy working across roles and disciplines. I’m looking for new opportunities to build the next generation (and beyond) of robotics. If you’re working on something interesting, or want to learn more, I’d love to chat!"
  },
  {
    "objectID": "index.html#projects-coming-soon",
    "href": "index.html#projects-coming-soon",
    "title": "Saeejith Nair",
    "section": "Projects (Coming soon!)",
    "text": "Projects (Coming soon!)\nCheck out my dedicated projects page!\n\n\n\n\n\n\n\n\nNAS-NeRF\nNV-Synth\nElastic-NeRF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDARLEI\nTurboViT"
  },
  {
    "objectID": "index.html#industry-experience",
    "href": "index.html#industry-experience",
    "title": "Saeejith Nair",
    "section": "Industry Experience",
    "text": "Industry Experience\n\nNuro.ai | Software Engineering Intern\n\nMountain View, CA (June - Sept, 2021)\nBrought up hardware accelerator, implemented efficient digital signal processing algorithms (in C) for radars on self-driving delivery robot, and developed testing architecture for radar HIL simulation chambers.\n\n\n\nBRAIN Lab | Undergraduate Researcher\n\nUniversity of Waterloo (May - Sept, 2020)\nDeveloped a rigid-body grasping simulator in PyBullet and used synthetic demonstration data to train a planner for robotic assembly tasks.\n\n\n\nNuro.ai | Software Engineering Intern \n\nMountain View, CA (Jan - May, 2020)\nDeveloped onboard and embedded communications infrastructure for radar driver module on self-driving delivery robot. Improved sensor data throughput and firmware upgrade speeds.\n\n\n\nArcturus Networks | Embedded Machine Vision Developer Intern\n\nToronto, ON (May - Aug, 2019)\nDesigned real-time, multithreaded video analysis application in C++ for low power embedded platform. Improved multi-object tracking algorithms and developed a library to accelerate CPU-based inference using ARM NEON intrinsics.\n\n\n\nCurtiss-Wright Defense Solutions | Embedded Software Engineering Intern\n\nOttawa, ON (Sept - Dec, 2018)\nImplemented, validated, and released features for U-Boot bootloaders. Drove debug and validation efforts across VxWorks & Yocto Linux BSPs, BIOS, and bootloader products to meet release schedule.\n\n\n\nCentre for Operational Research and Analysis, DRDC | Research Engineer\n\nOttawa, ON (Sept 2018 - Apr 2019)\nPrototyped tool for CANSOFCOM to optimize troop schedules using genetic algorithms.\n\n\n\nNXP Semiconductors | AI & Vision Processor System Level Design Intern\n\nOttawa, ON (Jan - Apr, 2018)\nDeveloped an internal tool to simulate DMA performance of their accelerated vision processor for ADAS applications. Analyzed results and waveforms to identify bugs in SystemVerilog RTL resulting in 2.73x increase in DMA bandwidth.\n\n\n\nHome Depot Canada | Junior Front End Web Developer\n\nToronto, ON (May - Aug, 2017)\nDeveloped features for an internal front-end component library to assist migrating the Home Depot Canada website to React."
  },
  {
    "objectID": "index.html#selected-publications",
    "href": "index.html#selected-publications",
    "title": "Saeejith Nair",
    "section": "Selected Publications",
    "text": "Selected Publications\n\nSaeejith Nair, Yuhao Chen, Mohammad Javad Shafiee, Alexander Wong. NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. arXiv:2309.14293, 2023. NewInML workshop @ NeurIPS, December 2023.\nSaeejith Nair, Javad Shafiee, Alexander Wong. DARLEI: Deep Accelerated Reinforcement Learning with Evolutionary Intelligence. arXiv:2312.05171, 2023. Conference on Vision and Intelligent Systems (CVIS), December 2023 (Oral).\nSaeejith Nair, Chi-en Amy Tai, Yuhao Chen, Alexander Wong. NutritionVerse-Synth: An Open Access Synthetically Generated 2D Food Scene Dataset for Dietary Intake Estimation. arXiv:2312.06192, 2023. NewInML workshop @ NeurIPS, December 2023.\nAlexander Wong, Saad Abbasi, Saeejith Nair. TurboViT: Generating Fast Vision Transformers via Generative Architecture Search. arXiv:2308.11421, August 2023. NewInML workshop @ NeurIPS, December 2023.\nChi-en Amy Tai, Matthew Keller, Saeejith Nair, Yuhao Chen, Yifan Wu, Olivia Markham, Krish Parmar, Pengcheng Xi, Heather Keller, Sharon Kirkpatrick, Alexander Wong. NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. Multimedia Assisted Dietary Management workshop @ ACM Multimedia Conference, September 2023.\nAlexander Wong, Yifan Wu, Saad Abbasi, Saeejith Nair, Javad Shafiee. Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge. Neural Architecture Search workshop @ CVPR, June 2023, pp. 2293-2297.\nChi-en Amy Tai, Matthew Keller, Mattie Kerrigan, Yuhao Chen, Saeejith Nair, Pengcheng Xi, Alexander Wong. NutritionVerse-3D: A 3D Food Model Dataset for Nutritional Intake Estimation. arXiv:2304.05619, Apr 2023. WiCV workshop @ CVPR, June 2023.\nAlexander Wong, Javad Shafiee, Saad Abbasi, Saeejith Nair, Mahmoud Famouri. Faster Attention Is What You Need: A Fast Self-Attention Neural Network Backbone Architecture for the Edge via Double-Condensing Attention Condensers. All Things Attention workshop @ NeurIPS, December 2022.\nSaeejith Nair, Saad Abbasi, Javad Shafiee, Alexander Wong. Maple-Edge: A Runtime Latency Predictor for Edge Devices. Embedded Vision workshop @ CVPR, June 2022 (Oral), pp. 3660-3668.\nXueyang Yao, Saeejith Nair, Peter Blouw, Bryan Tripp. Inferring symbols from demonstrations to support vector-symbolic planning in a robotic assembly task. hal-03041290, November 2020. SMILES (Sensorimotor Interaction, Language and Embodiment of Symbols) workshop @ ICDL, November 2020."
  },
  {
    "objectID": "index.html#pronunciation-guide",
    "href": "index.html#pronunciation-guide",
    "title": "Saeejith Nair",
    "section": "Pronunciation guide",
    "text": "Pronunciation guide\nMy first name is pronounced like sigh-jith (IPA: /ˈsaɪ-dʒɪθ/)."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Selected Projects",
    "section": "",
    "text": "Sorry, this page is currently under construction! 👷 In the meantime, please check out my GitHub page."
  },
  {
    "objectID": "projects/index.html#nas-nerf",
    "href": "projects/index.html#nas-nerf",
    "title": "Selected Projects",
    "section": "NAS-NeRF",
    "text": "NAS-NeRF\nNAS-NeRF: We introduce a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures by balancing architecture complexity and target synthesis quality metrics. Our method incorporates constraints on target metrics and budgets to guide the search towards architectures tailored for each scene. More about NAS-NeRF"
  },
  {
    "objectID": "projects/index.html#nv-synth",
    "href": "projects/index.html#nv-synth",
    "title": "Selected Projects",
    "section": "NV-Synth",
    "text": "NV-Synth\nNV-Synth: NutritionVerse-Synth, an open-access synthetically generated 2D food scene dataset, is a valuable resource for dietary intake estimation and food-related computer vision tasks. More about NV-Synth\n\n\n\n\n\n\n\nNV-Synth Dynamic Plating\nNV-Synth Procedural Plating"
  },
  {
    "objectID": "projects/index.html#darlei",
    "href": "projects/index.html#darlei",
    "title": "Selected Projects",
    "section": "DARLEI",
    "text": "DARLEI\n\nDARLEI: Deep Accelerated Reinforcement Learning with Evolutionary Intelligence combines evolutionary algorithms with parallelized reinforcement learning for efficiently training and evolving populations of UNIMAL agents. More about DARLEI"
  },
  {
    "objectID": "projects/index.html#turbovit",
    "href": "projects/index.html#turbovit",
    "title": "Selected Projects",
    "section": "TurboViT",
    "text": "TurboViT\n\nTurboViT: Generating Fast Vision Transformers via Generative Architecture Search. More about TurboViT"
  },
  {
    "objectID": "projects/index.html#maple-edge",
    "href": "projects/index.html#maple-edge",
    "title": "Selected Projects",
    "section": "Maple-Edge",
    "text": "Maple-Edge\n\nMaple-Edge: A runtime latency predictor for edge devices, focusing on improving the performance of AI applications in resource-constrained environments. More about Maple-Edge"
  },
  {
    "objectID": "blog/posts/research_workflows/index.html",
    "href": "blog/posts/research_workflows/index.html",
    "title": "Research Workflows",
    "section": "",
    "text": "Research is a lot of fun but I’ve found that without the right tools, a lot of annoyingly small issues can add up and become a source of friction over time. In this post, I’m going to try and document some of my existing strategies as well as plan out workflows for the remaining bottlenecks. Overall, there’s 5 broad areas which have high coefficients of friction (yes this is the entire research process but bear with me):"
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#archiving-and-organizing-literature",
    "href": "blog/posts/research_workflows/index.html#archiving-and-organizing-literature",
    "title": "Research Workflows",
    "section": "Archiving and Organizing Literature",
    "text": "Archiving and Organizing Literature\nWho needs Twitter’s Firehose API when you can get your own personal firehose of papers for the low, low price of being a terminally online doomscroller. I used to bookmark everything interesting I came across but since bookmarks are inherently unsearchable (why isn’t this a thing yet?? nvm while writing this, I found a Chrome product announcement from Dec 2022, so turns out you can indeed search through your bookmarks now), this quickly became a mountain of links that I never ended up revisiting. Regardless, the bottleneck is neither finding nor saving papers, but rather organizing them in a way that makes them easy to recall and consume. I’ve tried a number of different approaches over the years but I think I’ve finally settled on a workflow that works for me.\n\nSaving Papers\ntldr; Save to Readwise Reader inbox and immediately tag based on title and abstract.\nI’ve been using Readwise Reader for a few months now and it’s one of the few subscriptions I don’t regret paying for. The biggest benefit is that I can easily save links, papers, Twitter threads, or even YouTube videos to the mobile app (in 2 clicks) or with the Chrome extension on desktop (1 click) and it automatically syncs across devices. Since Reader is designed for reading, annotations are a first-class feature (even for PDFs), which means that I don’t necessarily have to wait until I get back to my desktop to read something I have saved. I can just read it on my phone and jot down annotations as I go along. The best part is that everything I annotate gets automatically pushed to my Obsidian knowledge base, which means that if I need to pull up a paper again in the future, I can just search for it in Obsidian and all my annotations will be there. I also try to add some tags as soon as I save something, and these get automatically exported to the Obsidian note as well, which makes it easier to find papers later on.\n\n\nOrganizing Papers\ntldr; Use Zotero to index the correct metadata for a paper and then export it to Obsidian where you can synthesize a short note based on your previous Readwise annotations.\nIf it seems like I’m doing double (triple?) the work by saving the same paper to Readwise, Obsidian, AND Zotero, yessir you’re absolutely correct. Unfortunately, c’est la vie and until someone builds a (free) app that does everything I want, I’m going to continue with this workflow. But fret not because this isn’t as bad as it sounds and it effectively addresses a number of different pain points.\nConsider the situation where let’s say CVPR acceptances have just come out and your firehose is full of “Thrilled to share” posts… that still link to an arXiv. Or maybe someone posts a thread to a neat finding they made and Reviewer 2 jumps in the replies to point out that this was already done by Schmidhuber in 1996… with a link to a PDF hosted on HAL. I love open dissemination of science as much as the next guy but until Zotero figures out a way to intelligently merge duplicate entries, I’m going to try and save the “official” version of the paper, so that I don’t have to manually update my BibLaTeX file a couple hours before whichever conference deadline I’m rushing to meet. That being said, I still want to save the link and quickly skim the paper I came across, which I can do with Reader. And at the end of the day, after I’ve finished annotating the saved copy or tagged and moved it from my Inbox tab into the Later tab on Reader, I use the Google Scholar extension to quickly find the published version of the paper and save the final version to Zotero (along with the correct metadata).\nOnce it’s saved to Zotero, I use ZotFile to automatically rename the PDF and reformat the metadata to create a citation key that’s both short, informative, and easy to remember. A few seconds after a paper is formatted correctly, a background service uses Zotero’s Export Library feature to add the newly indexed paper to a BibLaTeX-based mirror of my Zotero database. This file lives inside my Obsidian vault and is automatically backed up to my GitHub repo. I can then switch to my Obsidian vault and use a keyboard shortcut to trigger a refresh of the citation plugin which queries the updated database and creates a new templated note for the newly added paper, along with an #unread tag, which signifies that I haven’t properly synthesized a note for that paper yet. The nice thing about Obsidian is that the Dataview plugin lets me create a dashboard in my Daily Note showing all the papers I’ve saved to Zotero but haven’t read yet. This means that I can quickly skim through the list and decide which papers I want to read next. Once I’ve read a paper and synthesized a note, I can just remove the #unread tag and the paper will disappear from the dashboard."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#revisiting-literature",
    "href": "blog/posts/research_workflows/index.html#revisiting-literature",
    "title": "Research Workflows",
    "section": "Revisiting Literature",
    "text": "Revisiting Literature\ntldr; Unsure, check back later to see what I’ve converged on.\nI ended off the last section saying how the paper will disappear from the dashboard once you remove the tag. Unfortunately, the paper usually disappears from my memory as well, especially if it’s something that’s not directly related to my current research.\n\nLocal RAG-based Search?\nI don’t have a great solution for this yet, but over New Years I wanted to dig into all the new LLM/RAG methods that have come out so I got a simple app working that does RAG-style question answering based on the notes/papers in my Obsidian vault. I’ll have to write another post about it soon but I was pretty happy with the results, especially considering the fact that it was running locally (albeit still on a GPU). The next step is to get it running fully on-device on my M1 Macbook which I think should be doable based on some of my preliminary tests of Apple’s new MLX framework.\n\n\nImproved Spaced-Repetition?\nThere’s a few AI assistant plugins for Obsidian already but none of them seem to tick all of the boxes so at some point, I’d like to build my own plugin. Ideally it would be able to work with me to generate Anki cards for papers I’ve recently read and then provide additional context after I review each card, to remind me of neat tidbits I might’ve forgotten. I’d also like to be able to ask it questions about papers I’ve read in the past and have it pull up the relevant notes. I think this is doable with the current state of the art but I’ll have to do some more research to figure out the best way to go about it.\n\n\nSerendipity Based Research Recommender System?\nOne thing I’ve gotten hooked on recently though is the idea of using serendipity in recommender systems. I had some really exciting conversations with folks after the ALOE workshop at NeurIPS (completely unrelated to recommender systems) and over the holidays I serendipitously came across Ken Stanley’s new startup, Maven, which is building a new social network based on serendipity. I think there’s a lot of unexplored potential in this space and I think one area where it could be really useful is in the context of research. Imagine having an assistant that serendipitously reminds you of a paper or an idea that’s not directly related to your current research but makes connections between ideas in different fields, thereby giving you a fresh perspective on how to solve something you’re stuck on! Plus, since it’s an Obsidian plugin, the chances of it hallucinating something completely random would be pretty low because it’s grounded in your notes, and your TODO list, and is actually aware of what you’re working on a day-to-day basis."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#running-and-managing-experiments",
    "href": "blog/posts/research_workflows/index.html#running-and-managing-experiments",
    "title": "Research Workflows",
    "section": "Running and Managing Experiments",
    "text": "Running and Managing Experiments\nI often work on a number of different projects simultaneously and because they’re all research-y/open-ended, I end up running a lot of experiments and making iterative improvements. But with the number of things I try out, I don’t seem to have yet found a clean yet straightforward approach for keeping track of everything. So far, I’ve been parallelizing experiments across heterogeneous compute nodes using WandB sweeps, logging everything to their backend, and using WandB reports to share key findings with collaborators. This is a much better workflow than what I had when I originally started doing ML and I had to SSH into remote servers to inspect Tensorboard logs saved on each filesystem.\nWhile this setup had its frustrations and bottlenecks, it’s recently become unusable. Turns out that my WandB academic tier plan only supports 100GB of storage (which I’d surpassed months ago) and I am currently sitting at 3.1TB of logs/data that they’ve recently sent me an update about. I love WandB but there’s no way I’m paying them $90/month which means it’s time to be scrappy and build something ourselves. Honestly, I think as long as I can keep the per experiment data usage below the threshold (which should definitely be possible), it should be ok? Or even better, if I can get an on-premise, locally hosted version of the WandB server up and running, I can keep using as much storage as I want. The main problem is reliability - I don’t know if the servers we have on campus will be able to keep up and I don’t particularly fancy being a sysadmin. It’s honestly kind of wild when you think about the various challenges involved and look into how people solve it at scale, and turns out you either have some universities with their own massive IT department or startups burning their runway on other startups that provide niche monitoring solutions. If you have neither, you build it yourself. Onwards!\nAs I migrate to my own locally hosted experiment management system, I’ll progressively update this section with my findings."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#analyzing-and-communicating-results",
    "href": "blog/posts/research_workflows/index.html#analyzing-and-communicating-results",
    "title": "Research Workflows",
    "section": "Analyzing and Communicating Results",
    "text": "Analyzing and Communicating Results\ntldr; Use Quarto to create interactive reports with rich multimedia and LaTEX.\nEspecially now that I’m moving away from storing my data on WandB, I need a way to quickly pull the data I want, process it, and publish results/findings. I’ve also decided to start blogging about my intermediate results and creating polished figures as I go along, so that hopefully the process of writing the paper at the end is less painful as I can just copy-paste writing that I already published on my blog into the paper. No, this isn’t plagiarism, and if some handbook says it is, we ought to rewrite the handbook (but in the meantime I’ll just cite myself if you reeeeally want). Science (and publishing by extension) should be about sharing knowledge and ideas in a manner that is clear and reproducible, not about jumping through hoops to appease some arbitrary standard.\nI think Quarto is the way to go here. It’s a relatively new framework but it’s already got a lot of the features I want and I think it’s only going to get better over time. The community seems to love sharing reproducible demos and tutorials in Google Colab, and I think that’s fine. But I personally don’t think Colab (or regular Jupyter Notebooks) are a good interface for sharing results. It’s clunky (scrolling through a long notebook is an awful experience), aesthetically unpleasing (does not make use of screen space very well), difficult to style, and is not responsive on mobile. While tools like nbdev solve some of the problems related to using notebooks with version control, it hasn’t become mainstream yet (despite being around for a few years now) and most Colab notebooks today are still that – notebooks. Lastly, notebooks cannot be easily exported to other formats or embedded into the wider web ecosystem. I’m really excited that Quarto solves all of these problems and part of my goal in starting this blog is to figure out how to use it better and communicate my research more effectively with the wider community."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#writing-the-paper",
    "href": "blog/posts/research_workflows/index.html#writing-the-paper",
    "title": "Research Workflows",
    "section": "Writing the Paper",
    "text": "Writing the Paper\ntldr; (for now) Use Overleaf to collaborate with co-authors and write the paper.\nWhile most researchers use Overleaf today, it seems that many people use it on the free personal plan, without realizing that if you’re at a University, you (probably) have access to the Overleaf Premium tier for free. The only real difference between the two is that the Premium tier gives you more features for collaborating with co-authors, tracking changes, and longer revision history, but I think it’s quite worth it.\nWhile Overleaf is great, I find myself often annoyed at how long it takes to recompile the document after making a change. I’ve noticed this to be more of a problem if the document has a lot of images in it (gg computer vision) and I’m not sure if this is a limitation with how I use Overleaf or if it’s a problem that others have with the platform as well. It seems that compiling is faster if I do it locally but then I lose the benefits of Overleaf’s collaboration features. I’m not sure if there’s a way to get the best of both worlds but I’ll update this section if I find a solution."
  }
]