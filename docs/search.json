[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Streams of Consciousness",
    "section": "",
    "text": "GARField: Group Anything with Radiance Fields\n\n\n\n\n\n\nnerf\n\n\nwip\n\n\ngaussian_splatting\n\n\n\nNotes on the GARField paper.\n\n\n\n\n\nAug 6, 2024\n\n\nSaeejith Nair\n\n\n\n\n\n\n\n\n\n\n\n\nExploring 2D Gaussian Splatting\n\n\n\n\n\n\ngaussian_splatting\n\n\nresearch\n\n\nwip\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nSaeejith Nair\n\n\n\n\n\n\n\n\n\n\n\n\nElastic-NeRF\n\n\n\n\n\n\nnerf\n\n\nwip\n\n\nresearch\n\n\n\nMaking NeRFs even smaller and faster\n\n\n\n\n\nJan 9, 2024\n\n\nSaeejith Nair\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Workflows\n\n\n\n\n\n\npersonal\n\n\nproductivity\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nSaeejith Nair\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/elastic_nerf/index.html",
    "href": "blog/posts/elastic_nerf/index.html",
    "title": "Elastic-NeRF",
    "section": "",
    "text": "Elastic-NeRF applied to Nerfacc’s Instant-NGP implementation\n\n\nPrior to the holidays, I came across the MatFormer: Nested Transformer for Elastic Inference paper and thought it was super cool. I’ve been working on Neural Radiance Fields lately and applying generative architecture search to the radiance field in order to make it more compact and efficient and we got some pretty nice speedups and memory savings just from shrinking the MLPs (see NAS-NeRF). People have tried doing elastic supernets in NAS before, but it’s never been quite so simple as the MatFormer approach afaik, and their accuracy-performance characterization was really interesting. I wanted to see if it would transfer over to the NeRF domain, so I tried it out and it works insanely well!\nAlso the coolest part is that unlike MatFormer where they had to jointly optimize across all granularities on each forward pass, it turns out that for NeRFs you can just sample a single granularity and it works just as well! So training \\(N\\) granularities can actually be faster than training the biggest granularity.\nTime to run more experiments and characterize the training dynamics…"
  },
  {
    "objectID": "blog/posts/2d_gaussian_splatting/index.html",
    "href": "blog/posts/2d_gaussian_splatting/index.html",
    "title": "Exploring 2D Gaussian Splatting",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Selected Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nElastic-NeRF\n\n\n\n\n\n\nnerf\n\n\nelastic\n\n\nwip\n\n\n\nMaking NeRFs even more compact and efficient using an elastic architecture (and no additional training cost)!\n\n\n\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNAS-NeRF\n\n\n\n\n\n\nnerf\n\n\nnas\n\n\nneurips workshop\n\n\n\nWe introduce a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures.\n\n\n\n\n\nDec 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNV-Synth\n\n\n\n\n\n\nsynthetic data\n\n\nneurips workshop\n\n\n\nWe introduce a large-scale synthetic food image dataset for nutrition estimation. NV-Synth contains 84,984 photorealistic meal images rendered from 7,082 dynamically plated 3D scenes.\n\n\n\n\n\nDec 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDARLEI\n\n\n\n\n\n\nRL\n\n\nevolution\n\n\ncvis\n\n\n\nDARLEI combines evolutionary algorithms with parallelized reinforcement learning for efficiently training and evolving populations of UNIMAL agents.\n\n\n\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTurboViT\n\n\n\n\n\n\nvit\n\n\nnas\n\n\nneurips workshop\n\n\n\nWe explore the generation of fast vision transformer architecture designs via generative architecture search to achieve a strong balance between accuracy and architectural and computational efficiency.\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMaple-Edge\n\n\n\n\n\n\nedge-ml\n\n\nlatency prediction\n\n\ncvpr workshop\n\n\n\nWe propose MAPLE-Edge, an edge-device oriented latency predictor where we train a regression network on architecture-latency pairs in conjunction with a hardware-runtime descriptor to effectively estimate inference latency on a diverse pool of edge devices.\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Saeejith Nair",
    "section": "",
    "text": "I’m currently an MASc candidate in Systems Design Engineering at UWaterloo’s Vision and Image Processing Lab, advised by Prof. Alexander Wong and Prof. Javad Shafiee. My research has focused on projects at the intersection of efficient ML (specifically applying neural architecture search to NeRFs and vision transformers), embedded systems (latency prediction for on-device inference), and robotics (building simulation pipelines for synthetic data generation, training embodied morphologies, and dexterous manipulation). I’m interested in building embodied systems that can efficiently learn to interact with the world in an open-ended manner.\nI previously graduated from the University of Waterloo with a BASc in Mechatronics Engineering with an Option in Artificial Intelligence. As an undergraduate, I’ve interned as a software engineer across the full systems stack including sensor processing and communications infrastructure on self-driving cars, simulators for robot grasping, real-time video analytics applications for embedded microcontrollers, safety-critical bootloader firmware for aerospace products, silicon validation for AI accelerators, and web development for e-commerce.\nI’m currently looking for new opportunities to build the next generation (and beyond) of robotics. If you’re working on something interesting, or want to learn more, I’d love to chat!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Saeejith Nair",
    "section": "",
    "text": "I’m currently an MASc candidate in Systems Design Engineering at UWaterloo’s Vision and Image Processing Lab, advised by Prof. Alexander Wong and Prof. Javad Shafiee. My research has focused on projects at the intersection of efficient ML (specifically applying neural architecture search to NeRFs and vision transformers), embedded systems (latency prediction for on-device inference), and robotics (building simulation pipelines for synthetic data generation, training embodied morphologies, and dexterous manipulation). I’m interested in building embodied systems that can efficiently learn to interact with the world in an open-ended manner.\nI previously graduated from the University of Waterloo with a BASc in Mechatronics Engineering with an Option in Artificial Intelligence. As an undergraduate, I’ve interned as a software engineer across the full systems stack including sensor processing and communications infrastructure on self-driving cars, simulators for robot grasping, real-time video analytics applications for embedded microcontrollers, safety-critical bootloader firmware for aerospace products, silicon validation for AI accelerators, and web development for e-commerce.\nI’m currently looking for new opportunities to build the next generation (and beyond) of robotics. If you’re working on something interesting, or want to learn more, I’d love to chat!"
  },
  {
    "objectID": "index.html#recent-projects",
    "href": "index.html#recent-projects",
    "title": "Saeejith Nair",
    "section": "Recent Projects",
    "text": "Recent Projects\nCheck out my projects page for more!\n\n\n\n\n\n\n\n\n\n\nElastic-NeRF\n\n\n\nnerf\n\n\nelastic\n\n\nwip\n\n\n\nMaking NeRFs even more compact and efficient using an elastic architecture (and no additional training cost)!\n\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNAS-NeRF\n\n\n\nnerf\n\n\nnas\n\n\nneurips workshop\n\n\n\nWe introduce a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures.\n\n\n\nDec 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNV-Synth\n\n\n\nsynthetic data\n\n\nneurips workshop\n\n\n\nWe introduce a large-scale synthetic food image dataset for nutrition estimation. NV-Synth contains 84,984 photorealistic meal images rendered from 7,082 dynamically plated…\n\n\n\nDec 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDARLEI\n\n\n\nRL\n\n\nevolution\n\n\ncvis\n\n\n\nDARLEI combines evolutionary algorithms with parallelized reinforcement learning for efficiently training and evolving populations of UNIMAL agents.\n\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurboViT\n\n\n\nvit\n\n\nnas\n\n\nneurips workshop\n\n\n\nWe explore the generation of fast vision transformer architecture designs via generative architecture search to achieve a strong balance between accuracy and architectural…\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaple-Edge\n\n\n\nedge-ml\n\n\nlatency prediction\n\n\ncvpr workshop\n\n\n\nWe propose MAPLE-Edge, an edge-device oriented latency predictor where we train a regression network on architecture-latency pairs in conjunction with a hardware-runtime…\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#industry-experience",
    "href": "index.html#industry-experience",
    "title": "Saeejith Nair",
    "section": "Industry Experience",
    "text": "Industry Experience\n\nNuro.ai | Software Engineering Intern\n\nMountain View, CA (June - Sept, 2021)\nBrought up hardware accelerator, implemented efficient digital signal processing algorithms (in C) for radars on self-driving delivery robot, and developed testing architecture for radar HIL simulation chambers.\n\n\n\nBRAIN Lab | Undergraduate Researcher\n\nUniversity of Waterloo (May - Sept, 2020)\nDeveloped a rigid-body grasping simulator in PyBullet and used synthetic demonstration data to train a planner for robotic assembly tasks.\n\n\n\nNuro.ai | Software Engineering Intern \n\nMountain View, CA (Jan - May, 2020)\nDeveloped onboard and embedded communications infrastructure for radar driver module on self-driving delivery robot. Improved sensor data throughput and firmware upgrade speeds.\n\n\n\nArcturus Networks | Embedded Machine Vision Developer Intern\n\nToronto, ON (May - Aug, 2019)\nDesigned real-time, multithreaded video analysis application in C++ for low power embedded platform. Improved multi-object tracking algorithms and developed a library to accelerate CPU-based inference using ARM NEON intrinsics.\n\n\n\nCurtiss-Wright Defense Solutions | Embedded Software Engineering Intern\n\nOttawa, ON (Sept - Dec, 2018)\nImplemented, validated, and released features for U-Boot bootloaders. Drove debug and validation efforts across VxWorks & Yocto Linux BSPs, BIOS, and bootloader products to meet release schedule.\n\n\n\nCentre for Operational Research and Analysis, DRDC | Research Engineer\n\nOttawa, ON (Sept 2018 - Apr 2019)\nPrototyped tool for CANSOFCOM to optimize troop schedules using genetic algorithms.\n\n\n\nNXP Semiconductors | AI & Vision Processor System Level Design Intern\n\nOttawa, ON (Jan - Apr, 2018)\nDeveloped an internal tool to simulate DMA performance of NXP’s accelerated vision processor for ADAS applications. Analyzed results and waveforms to identify bugs in SystemVerilog RTL resulting in 2.73x increase in DMA bandwidth.\n\n\n\nHome Depot Canada | Junior Front End Web Developer\n\nToronto, ON (May - Aug, 2017)\nDeveloped features for an internal front-end component library to assist migrating the Home Depot Canada website to React."
  },
  {
    "objectID": "index.html#selected-publications",
    "href": "index.html#selected-publications",
    "title": "Saeejith Nair",
    "section": "Selected Publications",
    "text": "Selected Publications\n\nSaeejith Nair, Yuhao Chen, Mohammad Javad Shafiee, Alexander Wong. NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. arXiv:2309.14293, 2023. NewInML workshop @ NeurIPS, December 2023.\nSaeejith Nair, Javad Shafiee, Alexander Wong. DARLEI: Deep Accelerated Reinforcement Learning with Evolutionary Intelligence. arXiv:2312.05171, 2023. Conference on Vision and Intelligent Systems (CVIS), December 2023 (Oral).\nSaeejith Nair, Chi-en Amy Tai, Yuhao Chen, Alexander Wong. NutritionVerse-Synth: An Open Access Synthetically Generated 2D Food Scene Dataset for Dietary Intake Estimation. arXiv:2312.06192, 2023. NewInML workshop @ NeurIPS, December 2023.\nAlexander Wong, Saad Abbasi, Saeejith Nair. TurboViT: Generating Fast Vision Transformers via Generative Architecture Search. arXiv:2308.11421, August 2023. NewInML workshop @ NeurIPS, December 2023.\nChi-en Amy Tai, Matthew Keller, Saeejith Nair, Yuhao Chen, Yifan Wu, Olivia Markham, Krish Parmar, Pengcheng Xi, Heather Keller, Sharon Kirkpatrick, Alexander Wong. NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. Multimedia Assisted Dietary Management workshop @ ACM Multimedia Conference, September 2023.\nAlexander Wong, Yifan Wu, Saad Abbasi, Saeejith Nair, Javad Shafiee. Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge. Neural Architecture Search workshop @ CVPR, June 2023, pp. 2293-2297.\nChi-en Amy Tai, Matthew Keller, Mattie Kerrigan, Yuhao Chen, Saeejith Nair, Pengcheng Xi, Alexander Wong. NutritionVerse-3D: A 3D Food Model Dataset for Nutritional Intake Estimation. arXiv:2304.05619, Apr 2023. WiCV workshop @ CVPR, June 2023.\nAlexander Wong, Javad Shafiee, Saad Abbasi, Saeejith Nair, Mahmoud Famouri. Faster Attention Is What You Need: A Fast Self-Attention Neural Network Backbone Architecture for the Edge via Double-Condensing Attention Condensers. All Things Attention workshop @ NeurIPS, December 2022.\nSaeejith Nair, Saad Abbasi, Javad Shafiee, Alexander Wong. Maple-Edge: A Runtime Latency Predictor for Edge Devices. Embedded Vision workshop @ CVPR, June 2022 (Oral), pp. 3660-3668.\nXueyang Yao, Saeejith Nair, Peter Blouw, Bryan Tripp. Inferring symbols from demonstrations to support vector-symbolic planning in a robotic assembly task. hal-03041290, November 2020. SMILES (Sensorimotor Interaction, Language and Embodiment of Symbols) workshop @ ICDL, November 2020."
  },
  {
    "objectID": "index.html#pronunciation-guide",
    "href": "index.html#pronunciation-guide",
    "title": "Saeejith Nair",
    "section": "Pronunciation guide",
    "text": "Pronunciation guide\nMy first name is pronounced like sigh-jith (IPA: /ˈsaɪ-dʒɪθ/)."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html",
    "href": "blog/posts/research_workflows/index.html",
    "title": "Research Workflows",
    "section": "",
    "text": "Research is fun but without the right tools, a lot of annoyingly small issues can add up and become a source of friction over time. In this post, I’m going to try and document some of my existing strategies as well as plan out workflows for the remaining bottlenecks. Overall, there’s 5 broad areas which have high coefficients of friction (yes this is the entire research process but bear with me):"
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#archiving-and-organizing-literature",
    "href": "blog/posts/research_workflows/index.html#archiving-and-organizing-literature",
    "title": "Research Workflows",
    "section": "Archiving and Organizing Literature",
    "text": "Archiving and Organizing Literature\nWho needs Twitter’s Firehose API when you can get your own personal firehose of papers for the low, low price of being a terminally online doomscroller. I used to bookmark everything interesting I came across but since bookmarks are inherently unsearchable (why isn’t this a thing yet?? nvm while writing this, I found a Chrome product announcement from Dec 2022, so turns out you can indeed search through your bookmarks now), this quickly became a mountain of links that I never ended up revisiting. Regardless, the bottleneck is neither finding nor saving papers, but rather organizing them in a way that makes them easy to recall and consume. I’ve tried a number of different approaches over the years but I think I’ve finally settled on a workflow that works for me.\n\nSaving Papers\ntldr; Save to Readwise Reader inbox and immediately tag based on title and abstract.\nI’ve been using Readwise Reader for a few months now and it’s one of the few subscriptions I don’t regret paying for. The biggest benefit is that I can easily save links, papers, Twitter threads, or even YouTube videos to the mobile app (in 2 clicks) or with the Chrome extension on desktop (1 click) and it automatically syncs across devices. Since Reader is designed for reading, annotations are a first-class feature (even for PDFs), which means that I don’t necessarily have to wait until I get back to my desktop to read something I have saved. I can just read it on my phone and jot down annotations as I go along. The best part is that everything I annotate gets automatically pushed to my Obsidian knowledge base, which means that if I need to pull up a paper again in the future, I can just search for it in Obsidian and all my annotations will be there. I also try to add some tags as soon as I save something, and these get automatically exported to the Obsidian note as well, which makes it easier to find papers later on.\n\n\nOrganizing Papers\ntldr; Use Zotero to index the correct metadata for a paper and then export it to Obsidian where you can synthesize a short note based on your previous Readwise annotations.\nIf it seems like I’m doing double (triple?) the work by saving the same paper to Readwise, Obsidian, AND Zotero, yessir you’re absolutely correct. Unfortunately, c’est la vie and until someone builds a (free) app that does everything I want, I’m going to continue with this workflow. But fret not because this isn’t as bad as it sounds and it effectively addresses a number of different pain points.\nConsider the situation where let’s say CVPR acceptances have just come out and your firehose is full of “Thrilled to share” posts… that still link to an arXiv. Or maybe someone posts a thread to a neat finding they made and Reviewer 2 jumps in the replies to point out that this was already done by Schmidhuber in 1996… with a link to a PDF hosted on HAL. I love open dissemination of science as much as the next guy but until Zotero figures out a way to intelligently merge duplicate entries, I’m going to try and save the “official” version of the paper, so that I don’t have to manually update my BibLaTeX file a couple hours before whichever conference deadline I’m rushing to meet. That being said, I still want to save the link and quickly skim the paper I came across, which I can do with Reader. And at the end of the day, after I’ve finished annotating the saved copy or tagged and moved it from my Inbox tab into the Later tab on Reader, I use the Google Scholar extension to quickly find the published version of the paper and save the final version to Zotero (along with the correct metadata).\nOnce it’s saved to Zotero, I use ZotFile to automatically rename the PDF and reformat the metadata to create a citation key that’s both short, informative, and easy to remember. A few seconds after a paper is formatted correctly, a background service uses Zotero’s Export Library feature to add the newly indexed paper to a BibLaTeX-based mirror of my Zotero database. This file lives inside my Obsidian vault and is automatically backed up to my GitHub repo. I can then switch to my Obsidian vault and use a keyboard shortcut to trigger a refresh of the citation plugin which queries the updated database and creates a new templated note for the newly added paper, along with an #unread tag, which signifies that I haven’t properly synthesized a note for that paper yet. The nice thing about Obsidian is that the Dataview plugin lets me create a dashboard in my Daily Note showing all the papers I’ve saved to Zotero but haven’t read yet. This means that I can quickly skim through the list and decide which papers I want to read next. Once I’ve read a paper and synthesized a note, I can just remove the #unread tag and the paper will disappear from the dashboard."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#revisiting-literature",
    "href": "blog/posts/research_workflows/index.html#revisiting-literature",
    "title": "Research Workflows",
    "section": "Revisiting Literature",
    "text": "Revisiting Literature\ntldr; Unsure, check back later to see what I’ve converged on.\nI ended off the last section saying how the paper will disappear from the dashboard once you remove the tag. Unfortunately, the paper usually disappears from my memory as well, especially if it’s something that’s not directly related to my current research.\n\nLocal RAG-based Search?\nI don’t have a great solution for this yet, but over New Years I wanted to dig into all the new LLM/RAG methods that have come out so I got a simple app working that does RAG-style question answering based on the notes/papers in my Obsidian vault. I’ll have to write another post about it soon but I was pretty happy with the results, especially considering the fact that it was running locally (albeit still on a GPU). The next step is to get it running fully on-device on my M1 Macbook which I think should be doable based on some of my preliminary tests of Apple’s new MLX framework.\n\n\nImproved Spaced-Repetition?\nThere’s a few AI assistant plugins for Obsidian already but none of them seem to tick all of the boxes so at some point, I’d like to build my own plugin. Ideally it would be able to work with me to generate Anki cards for papers I’ve recently read and then provide additional context after I review each card, to remind me of neat tidbits I might’ve forgotten. I’d also like to be able to ask it questions about papers I’ve read in the past and have it pull up the relevant notes. I think this is doable with the current state of the art but I’ll have to do some more research to figure out the best way to go about it.\n\n\nSerendipity Based Research Recommender System?\nOne thing I’ve gotten hooked on recently though is the idea of using serendipity in recommender systems. I had some really exciting conversations with folks after the ALOE workshop at NeurIPS (completely unrelated to recommender systems) and over the holidays I serendipitously came across Ken Stanley’s new startup, Maven, which is building a new social network based on serendipity. I think there’s a lot of unexplored potential in this space and I think one area where it could be really useful is in the context of research. Imagine having an assistant that serendipitously reminds you of a paper or an idea that’s not directly related to your current research but makes connections between ideas in different fields, thereby giving you a fresh perspective on how to solve something you’re stuck on! Plus, since it’s an Obsidian plugin, the chances of it hallucinating something completely random would be pretty low because it’s grounded in your notes, and your TODO list, and is actually aware of what you’re working on a day-to-day basis."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#running-and-managing-experiments",
    "href": "blog/posts/research_workflows/index.html#running-and-managing-experiments",
    "title": "Research Workflows",
    "section": "Running and Managing Experiments",
    "text": "Running and Managing Experiments\nI often work on a number of different projects simultaneously and because they’re all research-y/open-ended, I end up running a lot of experiments and making iterative improvements. But with the number of things I try out, I don’t seem to have yet found a clean yet straightforward approach for keeping track of everything. So far, I’ve been parallelizing experiments across heterogeneous compute nodes using WandB sweeps, logging everything to their backend, and using WandB reports to share key findings with collaborators. This is a much better workflow than what I had when I originally started doing ML and I had to SSH into remote servers to inspect Tensorboard logs saved on each filesystem.\nWhile this setup had its frustrations and bottlenecks, it’s recently become unusable. Turns out that my WandB academic tier plan only supports 100GB of storage (which I’d surpassed months ago) and I am currently sitting at 3.1TB of logs/data that they’ve recently sent me an update about. I love WandB but there’s no way I’m paying them $90/month which means it’s time to be scrappy and build something ourselves. Honestly, I think as long as I can keep the per experiment data usage below the threshold (which should definitely be possible), it should be ok? Or even better, if I can get an on-premise, locally hosted version of the WandB server up and running, I can keep using as much storage as I want. The main problem is reliability - I don’t know if the servers we have on campus will be able to keep up and I don’t particularly fancy being a sysadmin. It’s honestly kind of wild when you think about the various challenges involved and look into how people solve it at scale, and turns out you either have some universities with their own massive IT department or startups burning their runway on other startups that provide niche monitoring solutions. If you have neither, you build it yourself. Onwards!\nAs I migrate to my own locally hosted experiment management system, I’ll progressively update this section with my findings."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#analyzing-and-communicating-results",
    "href": "blog/posts/research_workflows/index.html#analyzing-and-communicating-results",
    "title": "Research Workflows",
    "section": "Analyzing and Communicating Results",
    "text": "Analyzing and Communicating Results\ntldr; Use Quarto to create interactive reports with rich multimedia and LaTEX.\nEspecially now that I’m moving away from storing my data on WandB, I need a way to quickly pull the data I want, process it, and publish results/findings. I’ve also decided to start blogging about my intermediate results and creating polished figures as I go along, so that hopefully the process of writing the paper at the end is less painful as I can just copy-paste writing that I already published on my blog into the paper. No, this isn’t plagiarism, and if some handbook says it is, we ought to rewrite the handbook (but in the meantime I’ll just cite myself if you reeeeally want). Science (and publishing by extension) should be about sharing knowledge and ideas in a manner that is clear and reproducible, not about jumping through hoops to appease some arbitrary standard.\nI think Quarto is the way to go here. It’s a relatively new framework but it’s already got a lot of the features I want and I think it’s only going to get better over time. The community seems to love sharing reproducible demos and tutorials in Google Colab, and I think that’s fine. But I personally don’t think Colab (or regular Jupyter Notebooks) are a good interface for sharing results. It’s clunky (scrolling through a long notebook is an awful experience), aesthetically unpleasing (does not make use of screen space very well), difficult to style, and is not responsive on mobile. While tools like nbdev solve some of the problems related to using notebooks with version control, it hasn’t become mainstream yet (despite being around for a few years now) and most Colab notebooks today are still that – notebooks. Lastly, notebooks cannot be easily exported to other formats or embedded into the wider web ecosystem. I’m really excited that Quarto solves all of these problems and part of my goal in starting this blog is to figure out how to use it better and communicate my research more effectively with the wider community."
  },
  {
    "objectID": "blog/posts/research_workflows/index.html#writing-the-paper",
    "href": "blog/posts/research_workflows/index.html#writing-the-paper",
    "title": "Research Workflows",
    "section": "Writing the Paper",
    "text": "Writing the Paper\ntldr; (for now) Use Overleaf to collaborate with co-authors and write the paper.\nWhile most researchers use Overleaf today, it seems that many people use it on the free personal plan, without realizing that if you’re at a University, you (probably) have access to the Overleaf Premium tier for free. The only real difference between the two is that the Premium tier gives you more features for collaborating with co-authors, tracking changes, and longer revision history, but I think it’s quite worth it.\nWhile Overleaf is great, I find myself often annoyed at how long it takes to recompile the document after making a change. I’ve noticed this to be more of a problem if the document has a lot of images in it (gg computer vision) and I’m not sure if this is a limitation with how I use Overleaf or if it’s a problem that others have with the platform as well. It seems that compiling is faster if I do it locally but then I lose the benefits of Overleaf’s collaboration features. I’m not sure if there’s a way to get the best of both worlds but I’ll update this section if I find a solution."
  },
  {
    "objectID": "blog/posts/garfield/index.html",
    "href": "blog/posts/garfield/index.html",
    "title": "GARField: Group Anything with Radiance Fields",
    "section": "",
    "text": "GARField is a project released by some of the core folks behind many NeRF projects (including Nerfstudio). They introduce a method which enables decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs."
  },
  {
    "objectID": "blog/posts/garfield/index.html#pipeline",
    "href": "blog/posts/garfield/index.html#pipeline",
    "title": "GARField: Group Anything with Radiance Fields",
    "section": "Pipeline",
    "text": "Pipeline\nA high-level overview of their pipeline is as follows:\n\nMask Extraction\nApply SAM to each image and derive 2D segmentation masks at various scales. Specifically, they use SAM’s automatic mask generator which queries SAM in a grid of points and produces 3 candidate segmentation masks per query point. They then filter and deduplicate nearly identical masks to produce a list of mask candidates of multiple sizes which can overlap or include each other. Note that this process is done independently of viewpoint, producing masks which may not be consistent across views. The goal is to generate a hierarchy of groupings based on objects’ physical size.\n\n\nScale Estimation\nUsing the images, they partially train a radiance field and render a depth image from each training camera pose. Then using this depth image, for each mask, they consider the 3D points within the mask and pick the scale based on the extent of the points’ position distribution.\nFor example, the mask gets projected into 3D space based on the depth image and the scale is estimated as the diameter of the sphere enclosing the object.\n\n\nScale-Conditioned Affinity Field\nNext, they train a scale-conditioned affinity field.\n\nAffinity Field\nAn affinity field is a type of feature field in which the feature distance reflects points’ affinity (i.e. closeness). But what exactly does it mean for two objects in a scene to have high or low affinity? Their key insight is that this relationship is not absolute; rather it is a function of scale.\nFor example, consider an image of a plant. At a coarse scale, we want all the leaves in the plant to be grouped together, or have high affinity. On the other hand, at a fine scale, we want to distinguish between each leaf in the plant so the affinity between each leaf must be very low.\nHow can we imbue this understanding to a model? We need to train the model such that it understands that the desired grouping is a function of scale; i.e. we must condition the affinity field based on scale.\nThe authors say:\n\nScale-conditioning is a key component of GARField which allows consolidating inconsistent 2D mask candidates. The same point may be grouped in several ways depending on the granularity of the groupings desired. Scale-conditioning alleviates this inconsistency because it resolves ambiguity over which group a query should belong to. Under scale-conditioning, conflicting masks of the same point no longer fight each other during training, but rather can coexist in the same scene at different affinity scales.\n\nFormally, the scale-conditioned affinity field is defined as \\(f(x, s)\\) over a 3D point \\(x\\) and euclidean scale \\(s\\), similar to the formulation in their previous work, LERF. The output features are constrained to a unit hyper-sphere, and the affinity between two points at a scale is defined by \\(\\langle f(x_1, s), f(x_2, s) \\rangle\\). These features are then volumetrically rendered with a weighted average using the same rendering weights based on NeRF density to obtain a value on a per-ray basis.\n\n\n\nContrastive Supervision\nGARField uses contrastive learning to train the affinity field. Specifically, they supervise the field with a margin-based contrastive objective using a loss which, at a given scale, tries to pull features within the same group to be close, and another which pushes features in different groups apart.\nDuring training, consider sampling two rays \\(r_1, r_2\\) from masks within the same training image, with corresponding scales \\(s_1, s_2\\). They volumetrically render the scale-conditioned affinity features along each ray to obtain ray-level features \\(f_1\\) and \\(f_2\\). If the masks are the same (i.e. \\(s_1 = s_2\\)), the features are pulled together with L2 distance: \\(\\|f_1 - f_2\\|_2^2\\). If \\(s_1 \\neq s_2\\), the features are pushed apart: \\(\\max(0, m - \\|f_1 - f_2\\|_2)^2\\) where \\(m\\) is the lower bound distance, or margin. Note that this loss is only applied among rays sampled from the same image, since masks across different viewpoints have no correspondence.\nHowever, the authors note that the supervision provided by the previous contrastive losses is not sufficient to preserve hierarchy. For example, although an egg might be correctly grouped with the soup at scale 0.22, at larger scales like 0.5, it fragments apart.\nThis violates the two requirements a well-behaved affinity field must satisfy: transitivity and containment.\n\nTransitivity\nTransitivity means that if two points are mutually grouped with a third, they should themselves be grouped together.\n\n\nContainment\nContainment means that if two points are grouped at a small scale, they should be grouped together at higher scales.\nTo ensure these requirements are met, GARField employs continuous scale supervision and a containment auxiliary loss, instead of just employing the contrastive loss naively.\n\n\n\nContinuous scale supervision\nIf you only use the scales obtained from the 3D masks, the groups will only be defined at the discrete values where the masks were chosen. For example, consider a grape bunch. Each berry might correspond to a scale near \\(s_1\\) and the whole bunch might correspond to a scale near \\(s_2\\) but no objects exist in the middle between those two scales. This discontinuity results in grouping instability, since the scale supervision is defined sparsely.\nInstead, the authors propose augmenting the scale uniformly randomly between the current mask’s scale and the next smallest mask’s scale. If a ray’s mask is the smallest mask for the given viewpoint, then they interpolate the scale between 0 and \\(s_1\\). While the authors claim that this data augmentation strategy ensures continuous scale supervision throughout the field, it is unclear how this solves the problem of discontinuous jumps between groups of different scales.\nIn practice, this means that given masks \\(M_1, ..., M_n\\) and original scales \\(s_1, ..., s_n\\), during training, GARField will actually sample new values for the scales, such that \\(s_1' \\sim U(0, s_1)\\), \\(s_2' \\sim U(s_1, s_2)\\), \\(s_3' \\sim U(s_2, s_3)\\), etc.\n\n\nContainment Auxiliary Loss\nTo encourage containment such that small scale groups remain at larger scales, the authors propose a containment auxiliary loss. The intuition is that two grapes within the same cluster should also be grouped together at larger scales (i.e. the entire bunch). Thus, if two rays \\(r_1\\) and \\(r_2\\) are in the same mask with scale \\(s\\), then they should also be pulled together at any scale larger than \\(s\\).\nTo achieve this, the authors describe how at each training step, for the rays grouped together at scale \\(s\\), they additionally sample a larger scale \\(s' &gt; s\\) at which the rays are also pulled together. However, the paper is light on details about this strategy and it is unclear how the results of this loss are incorporated into the overall loss."
  },
  {
    "objectID": "blog/posts/garfield/index.html#training",
    "href": "blog/posts/garfield/index.html#training",
    "title": "GARField: Group Anything with Radiance Fields",
    "section": "Training",
    "text": "Training\nGARField is built using the Nerfstudio framework on top of the Nerfacto model by defining a separate output head for the grouping field. The grouping field is represented with a hashgrid with 24 layers, each with a feature dimension of 2. This is followed by a 4-layer MLP with 256 neurons and ReLU activations. The MLP takes the scale as an additional input, concatenating it with the hashgrid features. The resulting output embeddings (the feature vectors produced by the scale-conditioned affinity field) are \\(d = 256\\) dimensions.\nIt’s important to note that GARField maintains a clear separation between the affinity field and the traditional NeRF components. The affinity features (embeddings) and the RGB outputs from NeRF are treated as distinct representations within the model. This separation is implemented by ensuring that these two components do not share any weights in their respective neural networks.\nAdditionally, the gradients computed for updating the affinity field do not propagate to or influence the parameters responsible for generating the RGB outputs in the NeRF model. This separation in the gradient flow means that the optimization of the affinity field occurs independently from the optimization of the NeRF’s color prediction.\nTo manage the range of scales effectively, the authors implement two key strategies:\n\nThey cap the maximum scale at twice the extent of the camera positions used in capturing the scene. The “extent of cameras” refers to the maximum distance between any two camera positions. For instance, if the furthest apart cameras are 5 units away from each other, the maximum scale would be set to 10 units. This upper limit prevents the model from considering unreasonably large scales that exceed the scope of the captured scene.\nThey apply sklearn’s quantile transform to normalize the scale inputs based on the distribution of computed 3D mask scales. The quantile transform maps the original distribution of scale values onto a more uniform distribution. This is particularly useful because the original scale values might be unevenly distributed, with perhaps many small scales (e.g., 0.1-1.0) and fewer large scales (e.g., 10-100). By applying the quantile transform, these values are “spread out” more evenly, typically resulting in a distribution between 0 and 1.\n\nGARField begins training the grouping field after 2000 steps of NeRF optimization, giving geometry time to converge. To speed up training, they first volumetrically render the hash value, then use it as input to the MLP to obtain a ray feature. With this deferred rendering, the same ray can be queried at different scales with only one extra MLP call. They also normalize the result of volume rendering to unit norm before inputting to the MLP, and also normalize the individual hashgrid value for point-wise queries.\nThe authors note that preprocessing SAM masks takes around 3-10 minutes, followed by about 20 minutes for training on a GTX 4090 GPU.\n\nRay and Mask Sampling\nSimilar to standard NeRF training, GARField samples rays over which to compute losses. However, because GARField uses a contrastive loss within each train image, naively sampling pixels uniformly during training is inadequate to provide a training signal in each minibatch of rays. To ensure sufficient pairs in each train batch, they first sample \\(N\\) images, and sample \\(M\\) rays within each image. To balance the number of images as well as the number of point pairs for supervision, they sample 16 images and 256 points per image, resulting in 4096 samples per train iteration.\nRecall that each ray can belong to one of 3 mask groups. Thus at each train step, they need to choose a mask group for each sampled ray. To do this, they retain a mapping from pixels to mask labels throughout training, and at each train step, randomly select a mask for each ray from its corresponding list of masks.\nHowever, the mask selection is not uniformly random; there are two important caveats:\n\nThe probability of a mask being chosen is weighted inversely with the log of the mask’s 2D pixel area. This prevents large scales from dominating the sampling process, since larger masks can be chosen via more pixels.\nDuring mask selection, they coordinate the random scale chosen across rays in the same image to increase the probability of positive pairs. To do this, they sample a single value between 0 and 1 per image, and index into each pixel’s mask probability CDF with the same value, ensuring pixels which land within the same group are assigned the same mask. Otherwise, the loss gets dominated by pushing forces which destabilize training."
  },
  {
    "objectID": "blog/posts/garfield/index.html#hierarchical-decomposition",
    "href": "blog/posts/garfield/index.html#hierarchical-decomposition",
    "title": "GARField: Group Anything with Radiance Fields",
    "section": "Hierarchical Decomposition",
    "text": "Hierarchical Decomposition\nOnce GARField has optimized a scale-conditioned affinity field, it can generate a hierarchy of 3D groups, organized in a tree structure where each node can be broken into potential subgroups. This hierarchical decomposition is achieved through a recursive clustering process that decreases the scale for affinity, using HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise), a density-based clustering algorithm that doesn’t require a prior on the number of clusters.\n\nProcess Overview\nThe hierarchical decomposition process can be broken down into several key steps:\n\nInitialization: The hierarchy is initialized by globally clustering features at a large scale (s_max), which is set to 1.0 for all experiments. This corresponds to the extent of the input cameras’ positions. These initial clusters form the top-level nodes in the scene decomposition.\nRecursive Clustering: The process then iteratively reduces the scale by a fixed epsilon (set to 0.05 in the experiments) and runs HDBSCAN on each leaf node. If HDBSCAN returns more than one cluster for a given node, those clusters are added as children, and the process recurses on these new nodes.\nTermination: This recursive process continues until the scale reaches 0, at which point the procedure terminates, returning the complete hierarchical tree."
  },
  {
    "objectID": "blog/posts/garfield/index.html#experiments",
    "href": "blog/posts/garfield/index.html#experiments",
    "title": "GARField: Group Anything with Radiance Fields",
    "section": "Experiments",
    "text": "Experiments\nThe authors note that existing 3D scan datasets tend to focus on object-level scans (e.g. Google Scanned Objects, Partnet), are simulated (e.g. Contrastive Lift), or contain primarily indoor household scenes (e.g. Scannet). Since they want to assess GARField’s ability to decompose in-the-wild 3D scenes into hierarchical groups (which vary wideley in size and semantics), they instead use a wide variety of indoor and outdoor scenes from the Nerfstudio and LERF datasets, with special focus on scenes with significant object hierarchy.\n\nQualitative Scene Decomposition\nTo visualize the decomposition, the authors use Gaussian Splatting to query GARField’s affinity field at gaussian centers. This approach is chosen because gaussian splats are easier to segment in 3D compared to NeRFs.\nTwo types of hierarchical clustering results are visualized:\n\nGlobal clustering at a coarse scale, followed by selecting groups corresponding to a few objects and further decomposing them into subgroups at successively decreasing scales.\nTree decomposition, where a single object is selected from the global clustering and then broken down into a hierarchy of subparts.\n\nThe results show that GARField achieves high-fidelity 3D groupings across a wide range of scenes and objects, from man-made objects (like keyboards and Lego models) to complex natural objects (like plants). By varying the scale, GARField can separate objects at different levels of granularity, such as distinguishing between a succulent and its pot, or identifying individual components of a toy.\n\n\nQuantitative Hierarchy\n(WIP, coming soon…)"
  }
]