---
title: "Exploring 2D Gaussian Splatting"
date: "2024-07-31"
image: "/assets/posts/in_progress.jpg"
categories: [gaussian_splatting, research, wip]
---

Coming soon...
<!-- # 2D Gaussian Splatting: A Deep Dive into the Core Function

In this post, we're going to dissect the `generate_2D_gaussian_splatting` function, which is at the heart of a 2D Gaussian Splatting implementation. We'll go through each part of the function, explain what it does, and delve into the mathematics behind it.

## Function Signature and Input Parameters

```python
def generate_2D_gaussian_splatting(kernel_size, sigma_x, sigma_y, rho, coords, colours, image_size=(256, 256, 3), device="cpu"):
```

Let's break down these parameters:

- `kernel_size`: The size of the Gaussian kernel (an odd integer, typically).
- `sigma_x`, `sigma_y`: Control the spread of the Gaussian in x and y directions.
- `rho`: The correlation coefficient between x and y (-1 ≤ ρ ≤ 1).
- `coords`: The coordinates where to place the Gaussians.
- `colours`: The colors of each Gaussian.
- `image_size`: The size of the output image (default: 256x256 RGB).
- `device`: The device to perform computations on (CPU or GPU).

## Setting Up the Covariance Matrix

```python
batch_size = colours.shape[0]
sigma_x = sigma_x.view(batch_size, 1, 1)
sigma_y = sigma_y.view(batch_size, 1, 1)
rho = rho.view(batch_size, 1, 1)

covariance = torch.stack(
    [torch.stack([sigma_x**2, rho*sigma_x*sigma_y], dim=-1),
     torch.stack([rho*sigma_x*sigma_y, sigma_y**2], dim=-1)],
    dim=-2
)
```

Here, we're constructing the covariance matrix for each Gaussian in the batch. The covariance matrix is a fundamental concept in multivariate statistics, describing the shape and orientation of the Gaussian distribution.

For a 2D Gaussian, the covariance matrix $\Sigma$ is given by:

$$
\Sigma = \begin{bmatrix} 
\sigma_x^2 & \rho\sigma_x\sigma_y \\
\rho\sigma_x\sigma_y & \sigma_y^2 
\end{bmatrix}
$$

Where:
- $\sigma_x$ and $\sigma_y$ are the standard deviations in x and y directions, controlling the spread of the Gaussian.
- $\rho$ is the correlation coefficient, controlling the orientation of the Gaussian.

The covariance matrix encapsulates three key properties of the Gaussian:
1. The variances $\sigma_x^2$ and $\sigma_y^2$ along the main diagonal represent the spread in each direction.
2. The off-diagonal elements $\rho\sigma_x\sigma_y$ represent the covariance between x and y, determining the orientation of the Gaussian.
3. The determinant of $\Sigma$ is related to the overall area or spread of the Gaussian.

## Ensuring Positive Semi-Definiteness

```python
determinant = (sigma_x**2) * (sigma_y**2) - (rho * sigma_x * sigma_y)**2
if (determinant <= 0).any():
    raise ValueError("Covariance matrix must be positive semi-definite")
```

A valid covariance matrix must be positive semi-definite. This property ensures that the Gaussian distribution is well-defined and has non-negative variance in any direction.

Mathematically, for a 2x2 matrix, positive semi-definiteness is ensured if:

1. All diagonal elements are non-negative: $\sigma_x^2 \geq 0$ and $\sigma_y^2 \geq 0$
2. The determinant is non-negative: $\det(\Sigma) = \sigma_x^2\sigma_y^2 - (\rho\sigma_x\sigma_y)^2 \geq 0$

The second condition implies that $-1 \leq \rho \leq 1$, which aligns with the definition of correlation coefficient.

This check is crucial because an invalid covariance matrix could lead to undefined behavior in subsequent calculations, such as taking the square root of a negative number or inverting a singular matrix.

## Inverse Covariance Matrix

```python
inv_covariance = torch.inverse(covariance)
```

We calculate the inverse of the covariance matrix, which we'll need later for the Gaussian function. The inverse covariance matrix, often denoted as $\Sigma^{-1}$, plays a crucial role in the multivariate Gaussian probability density function.

For a 2x2 matrix, the inverse can be calculated analytically:

$$
\Sigma^{-1} = \frac{1}{\det(\Sigma)} \begin{bmatrix} 
\sigma_y^2 & -\rho\sigma_x\sigma_y \\
-\rho\sigma_x\sigma_y & \sigma_x^2 
\end{bmatrix}
$$

The inverse covariance matrix is used in the exponent of the Gaussian function and determines the shape and orientation of the level curves of the distribution.

## Creating the Kernel Grid

```python
start = torch.tensor([-5.0], device=device).view(-1, 1)
end = torch.tensor([5.0], device=device).view(-1, 1)
base_linspace = torch.linspace(0, 1, steps=kernel_size, device=device)
ax_batch = start + (end - start) * base_linspace

ax_batch_expanded_x = ax_batch.unsqueeze(-1).expand(-1, -1, kernel_size)
ax_batch_expanded_y = ax_batch.unsqueeze(1).expand(-1, kernel_size, -1)

xx, yy = ax_batch_expanded_x, ax_batch_expanded_y
```

This section creates a grid for our Gaussian kernel. We use a range of [-5, 5] for both x and y dimensions, which covers most of the significant area of a standard normal distribution.

The choice of [-5, 5] is based on the properties of the normal distribution:
- Approximately 68% of the distribution lies within 1 standard deviation of the mean.
- Approximately 95% lies within 2 standard deviations.
- Approximately 99.7% lies within 3 standard deviations.

By using a range of [-5, 5], we ensure that we capture virtually all of the Gaussian's significant values, even for Gaussians with larger standard deviations.

## Calculating the Gaussian Function

```python
xy = torch.stack([xx, yy], dim=-1)
z = torch.einsum('b...i,b...ij,b...j->b...', xy, -0.5 * inv_covariance, xy)
kernel = torch.exp(z) / (2 * torch.tensor(np.pi, device=device) * torch.sqrt(torch.det(covariance)).view(batch_size, 1, 1))
```

This is where we calculate the actual Gaussian function. The multivariate Gaussian probability density function is given by:

$$
f(x,y) = \frac{1}{2\pi|\Sigma|^{1/2}} \exp\left(-\frac{1}{2} [x \quad y] \Sigma^{-1} \begin{bmatrix} x \\ y \end{bmatrix}\right)
$$

Where $|\Sigma|$ is the determinant of the covariance matrix.

Let's break down this formula:

1. $\frac{1}{2\pi|\Sigma|^{1/2}}$ is the normalization factor, ensuring that the total probability over all space is 1.
2. $\exp(\cdot)$ is the exponential function.
3. $-\frac{1}{2} [x \quad y] \Sigma^{-1} \begin{bmatrix} x \\ y \end{bmatrix}$ is the quadratic form in the exponent, which determines the shape of the Gaussian.

The `torch.einsum` function is used for efficient batch matrix multiplication, computing this quadratic form for each point in our grid and for each Gaussian in the batch.

## Normalizing the Kernel

```python
kernel_max_1, _ = kernel.max(dim=-1, keepdim=True)
kernel_max_2, _ = kernel_max_1.max(dim=-2, keepdim=True)
kernel_normalized = kernel / kernel_max_2
```

We normalize the kernel by its maximum value. This ensures that the highest point of each Gaussian has a value of 1. This step is crucial for several reasons:

1. It ensures consistency across different Gaussians, regardless of their parameters.
2. It simplifies the process of combining multiple Gaussians, as they all have the same peak value.
3. It allows for easier control of the Gaussian's contribution to the final image through color multiplication.

## Preparing the Kernel for RGB

```python
kernel_reshaped = kernel_normalized.repeat(1, 3, 1).view(batch_size * 3, kernel_size, kernel_size)
kernel_rgb = kernel_reshaped.unsqueeze(0).reshape(batch_size, 3, kernel_size, kernel_size)
```

Here, we're preparing the kernel for RGB colors by repeating it three times (for R, G, and B channels) and reshaping. This step allows us to apply different colors to each Gaussian later in the process.

## Padding the Kernel

```python
pad_h = image_size[0] - kernel_size
pad_w = image_size[1] - kernel_size
if pad_h < 0 or pad_w < 0:
    raise ValueError("Kernel size should be smaller or equal to the image size.")

padding = (pad_w // 2, pad_w // 2 + pad_w % 2,  # padding left and right
           pad_h // 2, pad_h // 2 + pad_h % 2)  # padding top and bottom
kernel_rgb_padded = torch.nn.functional.pad(kernel_rgb, padding, "constant", 0)
```

We pad the kernel to match the desired image size. This allows us to place the Gaussian anywhere in the image. The padding is crucial because:

1. It ensures that the Gaussian can be centered at any pixel in the image, including those near the edges.
2. It maintains the full shape of the Gaussian, even when it's placed near the image boundaries.
3. It allows for efficient batch processing of multiple Gaussians simultaneously.

## Translating the Kernel: A Deep Dive into Affine Transformations

In this section, we'll explore how the Gaussian kernels are positioned in the image using affine transformations. This is a crucial step in the Gaussian Splatting process, as it allows us to place each Gaussian at its specified location efficiently.

### Mathematical Background: Affine Transformations

An affine transformation is a linear mapping method that preserves points, straight lines, and planes. In 2D, it can be represented as a 2x3 matrix:

$$
\begin{bmatrix} 
a & b & t_x \\
c & d & t_y
\end{bmatrix}
$$

This matrix can represent scaling, rotation, shearing, and translation. In our case, we're primarily interested in translation, so our matrix simplifies to:

$$
\begin{bmatrix} 
1 & 0 & t_x \\
0 & 1 & t_y
\end{bmatrix}
$$

Where $(t_x, t_y)$ represents the translation in x and y directions.

### Code Breakdown

Let's break down the code and explain each part:

```python
b, c, h, w = kernel_rgb_padded.shape
theta = torch.zeros(b, 2, 3, dtype=torch.float32, device=device)
theta[:, 0, 0] = 1.0
theta[:, 1, 1] = 1.0
theta[:, :, 2] = coords
```

1. `b, c, h, w = kernel_rgb_padded.shape`: This extracts the dimensions of our padded kernel. `b` is the batch size (number of Gaussians), `c` is the number of channels (3 for RGB), and `h` and `w` are the height and width of the image.

2. `theta = torch.zeros(b, 2, 3, dtype=torch.float32, device=device)`: This creates a batch of 2x3 affine transformation matrices, initialized to zero.

3. `theta[:, 0, 0] = 1.0` and `theta[:, 1, 1] = 1.0`: This sets the scaling factors to 1, ensuring no scaling occurs.

4. `theta[:, :, 2] = coords`: This is where the translation happens. `coords` is a tensor of shape `(b, 2)`, where each row represents the (x, y) coordinates where a Gaussian should be placed. These values are placed in the last column of `theta`, corresponding to $t_x$ and $t_y$ in our affine transformation matrix.

### Affine Grid and Grid Sampling

```python
grid = F.affine_grid(theta, size=(b, c, h, w), align_corners=True)
kernel_rgb_padded_translated = F.grid_sample(kernel_rgb_padded, grid, align_corners=True)
```

These two lines are where the actual transformation occurs:

1. `F.affine_grid(theta, size=(b, c, h, w), align_corners=True)`:
   - This function takes our batch of affine transformation matrices (`theta`) and the desired output size.
   - It generates a sampling grid for each input in the batch.
   - The sampling grid tells us, for each output pixel, where we should sample from in the input image.
   - `align_corners=True` ensures that the extreme values (-1 and 1) in the generated grid correspond to the centers of the corner pixels of the input and output.

2. `F.grid_sample(kernel_rgb_padded, grid, align_corners=True)`:
   - This function applies the sampling grid to our input tensor (`kernel_rgb_padded`).
   - For each point in the output tensor, it computes the input value using bilinear interpolation of the values at the four nearest pixels.
   - The result is our translated Gaussian kernels.

# Affine Grid and Grid Sampling: A Step-by-Step Explanation

Let's break down the concepts of affine grid and grid sampling to their most basic elements, using a very simple example.

## 1. The Problem We're Solving

Imagine we have a 3x3 image of a smiley face:

```
[[ 0,  0,  0],
 [ 0,  1,  0],
 [ 1,  0,  1]]
```

We want to move this image one pixel to the right. Our desired output would be:

```
[[ 0,  0,  0],
 [ 0,  0,  1],
 [ 0,  1,  0]]
```

How can we achieve this using affine grid and grid sampling?

## 2. Affine Grid (F.affine_grid)

### What it does:
`F.affine_grid` creates a grid that tells us, for each pixel in our output image, where we should look in the input image to get the color.

### Step-by-step:

a) First, we create our transformation matrix. To move one pixel right, we use:
   ```
   [[1, 0, -1],
    [0, 1,  0]]
   ```
   The -1 in the top-right means "move left by 1 pixel", which effectively moves the image right.

b) We call `F.affine_grid` with this matrix and our desired output size (3x3).

c) It returns a grid like this:
   ```
   [[[[-1, -1, -1],  # x-coordinates
     [-2, -2, -2],
     [-3, -3, -3]],

    [[-1, -2, -3],  # y-coordinates
     [-1, -2, -3],
     [-1, -2, -3]]]]
   ```

d) What does this grid mean?
   - For the pixel at (0,0) in our output, look at (-1,-1) in our input.
   - For the pixel at (0,1) in our output, look at (-1,-2) in our input.
   - And so on...

## 3. Grid Sampling (F.grid_sample)

### What it does:
`F.grid_sample` uses the grid we just created to actually move the pixels from the input to the output.

### Step-by-step:

a) For each point in our grid, `F.grid_sample` looks at the corresponding location in the input image.

b) If the coordinates are exactly on a pixel, it just takes that pixel's value.

c) If the coordinates are between pixels, it interpolates (takes a weighted average of nearby pixels).

d) In our example:
   - Output (0,0) looks at input (-1,-1). This is outside our image, so it's treated as 0.
   - Output (0,1) looks at input (-1,-2). Also outside, so 0.
   - Output (1,1) looks at input (-2,-2). Outside, so 0.
   - Output (1,2) looks at input (-2,-3). This corresponds to input (1,0), which has value 1.
   - And so on...

e) After doing this for all pixels, we get our desired output:
   ```
   [[ 0,  0,  0],
    [ 0,  0,  1],
    [ 0,  1,  0]]
   ```

## 4. Why Use This Approach?

1. **Flexibility**: This method can handle any kind of 2D transformation, not just simple translations.

2. **Smooth Transformations**: The interpolation allows for smooth movement, even when we're not moving by whole pixel amounts.

3. **Efficiency**: This process happens for all pixels simultaneously, which is very fast on GPUs.

## 5. Back to Gaussian Splatting

In our Gaussian Splatting code:

```python
grid = F.affine_grid(theta, size=(b, c, h, w), align_corners=True)
kernel_rgb_padded_translated = F.grid_sample(kernel_rgb_padded, grid, align_corners=True)
```

- `theta` contains the transformation matrices for all our Gaussians.
- `F.affine_grid` creates a grid for each Gaussian.
- `F.grid_sample` then moves each Gaussian according to its grid.

The `align_corners=True` parameter is a technical detail that affects how the extreme values in the grid are interpreted. It ensures that the corners of the input and output are aligned.

### Mathematical Interpretation

What's happening here can be thought of as a coordinate transformation. Let's say we have a point $(x, y)$ in our original kernel. The affine transformation applies as follows:

$$
\begin{bmatrix} 
x' \\
y' \\
1
\end{bmatrix} = 
\begin{bmatrix} 
1 & 0 & t_x \\
0 & 1 & t_y \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix} 
x \\
y \\
1
\end{bmatrix}
$$

Resulting in:
$$
x' = x + t_x \\
y' = y + t_y
$$

This effectively moves each point in our kernel by $(t_x, t_y)$, which are the coordinates specified in `coords`.


## Applying Colors and Combining

```python
rgb_values_reshaped = colours.unsqueeze(-1).unsqueeze(-1)
final_image_layers = rgb_values_reshaped * kernel_rgb_padded_translated
final_image = final_image_layers.sum(dim=0)
final_image = torch.clamp(final_image, 0, 1)
final_image = final_image.permute(1,2,0)
```

Finally, we apply the specified colors to each Gaussian, sum all the Gaussians to create the final image, clamp the values between 0 and 1, and rearrange the dimensions to match the expected image format.

This step is where the actual "splatting" occurs:
1. Each Gaussian is multiplied by its corresponding color.
2. All colored Gaussians are summed together to form the final image.
3. The values are clamped to ensure they're in the valid range for image representation.

## Conclusion

This function is the core of 2D Gaussian Splatting. It takes a set of Gaussians defined by their positions, shapes, and colors, and renders them onto an image. The key steps are:

1. Defining the Gaussian kernels based on the covariance matrices.
2. Creating a grid and calculating the Gaussian function on this grid.
3. Normalizing and preparing the kernels for RGB colors.
4. Translating the kernels to their specified positions in the image.
5. Applying colors and combining all Gaussians to form the final image.

The power of this approach lies in its ability to represent complex images with a relatively small number of parameters (position, shape, and color of each Gaussian). By optimizing these parameters, we can create efficient and flexible representations of images.

In a full Gaussian Splatting system, this function would be used within a larger optimization loop that adjusts the Gaussian parameters to best represent a target image. The optimization process might also involve adding or removing Gaussians as needed to improve the representation. -->